{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's show equation (1) : $\\mathcal{l(\\theta) } = \\frac{1}{n}\\sum_{i=1}^n \\log (1+e^{ - y_ix_{i}^{T} \\theta}) $ \\\\\n",
        "\\\n",
        "$\n",
        "\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "        \\frac{1}{1+e^{-x_{i}^{T} \\theta}} & \\mbox{if } y_i = 1 \\\\\n",
        "        \\frac{1}{1+e^{x_{i}^{T} \\theta}} & \\mbox{if } y_i = -1\n",
        "    \\end{array}\n",
        "\\right.\n",
        "\\implies\n",
        "\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "        \\frac{1}{1+e^{-x_{i}^{T} \\theta}} & \\mbox{if } y_i = 1 \\\\\n",
        "        \\frac{1}{1+e^{x_{i}^{T} \\theta}} & \\mbox{if } y_i = -1\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$ \\\\\n",
        "\\\n",
        "$$\n",
        "\\mathbb{P}(y_i|x_i;\\theta) = \\frac{1}{1+e^{ - y_ix_{i}^{T} \\theta}} \\space \\text{ then, }\n",
        "\\mathbb{P}(Y|X;\\theta) = \\prod_{i=1}^n \\frac{1}{1+e^{ - y_ix_{i}^{T} \\theta}}\n",
        "$$\n",
        "$$\n",
        "\\log \\mathbb{P}(Y|X;\\theta) = \\log( \\prod_{i=1}^n \\frac{1}{1+e^{ - y_ix_{i}^{T} \\theta}}) \n",
        "\\implies\n",
        "\\log \\mathbb{P}(Y|X;\\theta) = \\sum_{i=1}^n \\log (\\frac{1}{1+e^{ - y_ix_{i}^{T} \\theta}}) \n",
        "$$\n",
        "$$\n",
        "\\implies\n",
        "\\log \\mathbb{P}(Y|X;\\theta) = - \\sum_{i=1}^n \\log (1+e^{ - y_ix_{i}^{T} \\theta})\n",
        "\\implies\n",
        "- \\log \\mathbb{P}(Y|X;\\theta) =  \\sum_{i=1}^n \\log (1+e^{ - y_ix_{i}^{T} \\theta})\n",
        "$$\n",
        "$$\n",
        "\\fbox{ $ \\mathcal{l(\\theta) } = \\frac{1}{n}\\sum_{i=1}^n \\log (1+e^{ - y_ix_{i}^{T} \\theta}) $ }\n",
        "$$ \\\\\n",
        "\\\n",
        "$$\n",
        "\\frac{\\partial l(\\theta)}{\\partial \\theta} = - \\frac{1}{n}  \\sum_{i=1}^n \\frac{y_ix_{i}}{1+e^{ y_ix_{i}^{T} \\theta}} \n",
        "$$ \\\\\n",
        "to update the parameters of our model we use the formula:\n",
        "$$\n",
        "\\theta = \\theta + \\frac{1}{n}  \\sum_{i=1}^n \\frac{y_ix_{i}}{1+e^{ y_ix_{i}^{T} \\theta}}\n",
        "$$"
      ],
      "metadata": {
        "id": "zUae5liEIZwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "7qpvuA5SB7EZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "gJI3KIdd_Vc6"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "epsilon = 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "awVPEni7B0rh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "yH0OpBK-_Vc8"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "labels = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "X7LtE6WZBwAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "-dZSRyZO_Vc_"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=True):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        '''to add a biais '''\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "    \n",
        "    def __sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def __loss(self, h,y):\n",
        "        \"\"\"the loss function of the equation(1)\"\"\"\n",
        "        return -np.sum(np.log(h))/ y.size\n",
        "\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        y=y.reshape(-1,1)\n",
        "\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self.theta = np.zeros((X.shape[1],1))\n",
        "\n",
        "        for i in range(self.num_iter):\n",
        "            z = X @ self.theta\n",
        "            h = self.__sigmoid(-z*y)\n",
        "\n",
        "            gradient = ((X*y).T @ h) / y.size\n",
        "            self.theta += self.lr * gradient\n",
        "            loss = self.__loss(self.__sigmoid(z*y),y)\n",
        "                \n",
        "            if(self.verbose ==True and i % 100 == 0):\n",
        "                print(f'loss: {loss} \\t')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training "
      ],
      "metadata": {
        "id": "S9kR1t3XBrge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzLmWF5x_VdB",
        "outputId": "be022aca-433e-404c-d6b9-3438882724b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.6931471805599453 \t\n",
            "loss: 0.05585423996945716 \t\n",
            "loss: 0.029977142628093017 \t\n",
            "loss: 0.02078039554786432 \t\n",
            "loss: 0.016016391951014432 \t\n",
            "loss: 0.01308524424192899 \t\n",
            "loss: 0.011092352617869396 \t\n",
            "loss: 0.009645614536444178 \t\n",
            "loss: 0.008545491837373747 \t\n",
            "loss: 0.007679496405965182 \t\n",
            "CPU times: user 60.7 ms, sys: 166 µs, total: 60.8 ms\n",
            "Wall time: 60.7 ms\n",
            "loss: 0.6931471805599453 \t\n",
            "loss: 0.5745999875627902 \t\n",
            "loss: 0.5630583801192413 \t\n",
            "loss: 0.5543809058800406 \t\n",
            "loss: 0.5477609309709068 \t\n",
            "loss: 0.5426523347684292 \t\n",
            "loss: 0.5386625528692957 \t\n",
            "loss: 0.5355082934014916 \t\n",
            "loss: 0.5329837517303795 \t\n",
            "loss: 0.5309381992028918 \t\n",
            "CPU times: user 54.6 ms, sys: 134 µs, total: 54.7 ms\n",
            "Wall time: 53.7 ms\n",
            "loss: 0.6931471805599453 \t\n",
            "loss: 0.30010470718450466 \t\n",
            "loss: 0.2479547651271465 \t\n",
            "loss: 0.2177897711282658 \t\n",
            "loss: 0.19713583039225266 \t\n",
            "loss: 0.18191776216157848 \t\n",
            "loss: 0.17017619426590935 \t\n",
            "loss: 0.16081041124403608 \t\n",
            "loss: 0.15314586551266152 \t\n",
            "loss: 0.14674397210043072 \t\n",
            "CPU times: user 92.6 ms, sys: 3.82 ms, total: 96.4 ms\n",
            "Wall time: 103 ms\n"
          ]
        }
      ],
      "source": [
        "#split data in train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=2,stratify=labels)\n",
        "\n",
        "# We apply OneVsAll\n",
        "list_theta = []\n",
        "u = np.unique(labels)\n",
        "for i in u:\n",
        "    X= X_train\n",
        "    y = y_train\n",
        "    # if label = 0 encodes y-->1 and for labels =/= 0 encodes y--> -1  for i = 0 and so on\n",
        "    y=np.where((y==i),1,-1)\n",
        "    model = LogisticRegression(lr=0.1, num_iter=1000)\n",
        "    %time model.fit(X, y)\n",
        "    list_theta.append(model.theta) #For each encoding, we collect its classifier (the calculated theta). \n",
        "theta = np.hstack(list_theta) #We put them in the theta so that if we want to predict, we choose the classifier with the highest confidence. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "g2FwZ6UaAVF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have three theta of each classifier and to get a prediction, we choose the label with the highest probability.\n",
        "def predict(X,theta):\n",
        "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "    return np.dot(X,theta).argmax(1).reshape(-1,1)\n"
      ],
      "metadata": {
        "id": "Xm1wK0r5TMJf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(X_test,theta)"
      ],
      "metadata": {
        "id": "beu7JI6oELHC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance"
      ],
      "metadata": {
        "id": "fNC-Zb05BjE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculation of precision and recall\n",
        "#Because the data is asymmetric, we are forced to use recall and precision to measure the performance of our model.\n",
        "precision = precision_score(y_test, preds,average = 'micro')\n",
        "recall = recall_score(y_test, preds,average = 'micro')\n",
        "\n",
        "print('Precision: ',precision)\n",
        "print('Recall: ',recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSB1bem1ucd0",
        "outputId": "d0b69f60-44cb-4851-dfbe-0c1677949504"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  0.98\n",
            "Recall:  0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A6AE7pY_VdC",
        "outputId": "b77258b6-2480-409d-c8cd-74d4c4729a5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17,  0,  0],\n",
              "       [ 0, 16,  1],\n",
              "       [ 0,  0, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "confusion_matrix(y_test, preds)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Alioune_B_M_DIANKHA_Resit_of foundation of Machine_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}